{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea7399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tarfile\n",
    "import json\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b38e724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#keywords to filter the tweets\n",
    "keywords=['gmo','gmos','gm food','gm foods','transgenic', 'transgenics','genetically-modified','genetically modified']\n",
    "\n",
    "# initializing array which will hold the filtered tweets\n",
    "filteredTweets=[]\n",
    "\n",
    "\n",
    "# opening the tar file\n",
    "with tarfile.open('data/May.tar', 'r') as tar:\n",
    "    \n",
    "    # iterating through all files and directories\n",
    "    for member in tar.getmembers():\n",
    "        \n",
    "        #skipping directories\n",
    "        if member.isdir():\n",
    "            continue\n",
    "        \n",
    "        #looking for the target files json.bz2\n",
    "        if member.name.endswith('.json.bz2'):\n",
    "            \n",
    "            # extracting each file\n",
    "            file = tar.extractfile(member)\n",
    "         \n",
    "            #reading in and decompressing the file\n",
    "            noBz2 = bz2.decompress(file.read()).decode('utf-8')\n",
    "            \n",
    "            # handling each tweet to find the match\n",
    "            for line in noBz2.splitlines():\n",
    "                \n",
    "                #each line is one tweet in json format, therefore loading line by line\n",
    "                tweet = json.loads(line)\n",
    "\n",
    "                # making sure the tweet has the fields 'text' and 'created_at'. If not they'll be skipped\n",
    "                if 'text' not in tweet or 'created_at' not in tweet or tweet['user']['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                # Skipping retweets\n",
    "                if tweet['text'].startswith('RT') and tweet.get('retweeted_status') is not None:\n",
    "                    continue\n",
    "                \n",
    "                # storing fields if present\n",
    "                tweetTxt = tweet['text']\n",
    "                created_at = tweet['created_at']\n",
    "\n",
    "                # Searching matches in text, splitting by words, lower case like keywords\n",
    "                if any(keyword in tweetTxt.lower().split() for keyword in keywords):                \n",
    "                    # Add the matching tweet to the list\n",
    "                    filteredTweets.append({\n",
    "                        'text': tweetTxt,\n",
    "                        'created_at': created_at\n",
    "                    })\n",
    "                    \n",
    "\n",
    "#writting the fields from matching tweets to a json output file\n",
    "with open('tweets.json', 'w') as output:\n",
    "    json.dump(filteredTweets, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82bedd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d34e8956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cacu-VirtualBox.mshome.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5f5ad2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+\n",
      "|         created_at|                text|\n",
      "+-------------------+--------------------+\n",
      "|2012-01-02 20:09:42|USDA has settled ...|\n",
      "|2012-01-03 02:41:36|Sounds INSANE. GM...|\n",
      "|2012-01-03 06:57:08|Petition: Tell th...|\n",
      "|2012-01-03 12:30:19|Mandatory GMO foo...|\n",
      "|2012-01-03 14:11:44|http://t.co/TI9oL...|\n",
      "|2012-01-03 16:10:27|Want to learn mor...|\n",
      "|2012-01-03 17:53:55|Safety assessment...|\n",
      "|2012-01-03 18:30:38|GMO labeling effo...|\n",
      "|2012-01-03 20:04:57|Leaked: US to Sta...|\n",
      "|2012-01-04 00:22:12|Leaked: US to Sta...|\n",
      "|2012-01-04 13:37:46|@IPIMCIMCIM  GMOs...|\n",
      "|2012-01-05 02:42:31|@Kaybaebaeeee gmo...|\n",
      "|2012-01-05 06:10:28|Apel ws. legislac...|\n",
      "|2012-01-05 09:36:54|Woman files class...|\n",
      "|2012-01-05 16:14:15|Leaked: US to Sta...|\n",
      "|2012-01-05 17:42:20|@harjxo GMO twidd...|\n",
      "|2012-01-05 19:42:18|@songbirdtiff Yup...|\n",
      "|2012-01-05 21:07:46|Label GMO food ca...|\n",
      "|2012-01-06 00:21:13|Transgenic Trees ...|\n",
      "|2012-01-06 01:31:14|Woman Sues Frito-...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#funtion to convert to time stamp with udf\n",
    "def parseDate(date_str):\n",
    "    return datetime.datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "parse_date_udf= udf(parseDate, TimestampType())\n",
    "\n",
    "#reading in the monthly json files\n",
    "tweets= spark.read.json('hdfs://localhost:9000/user1/tweet*.json')\n",
    "\n",
    "# applying udf to column\n",
    "tweets= tweets.withColumn('created_at', parse_date_udf(tweets['created_at']))\n",
    "\n",
    "#sorting by timestamp\n",
    "tweets= tweets.orderBy('created_at')\n",
    "\n",
    "tweets.printSchema();tweets.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e926704d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7e40f592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2012, 1, 2, 20, 9, 42), text='USDA has settled upon a superb solution: let the GMO industry conduct its own environmental impact tests. http://t.co/Ed5BbDRb')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample\n",
    "tweets.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "849de22b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5583ec38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dayofmonth(created_at): int, count(created_at): bigint]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupBy(dayofmonth('created_at')).agg(count('created_at'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0c8cc729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sSRIdY1HZCS",
    "outputId": "ab15e23e-0924-42bb-c694-f4ddb58df4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "2023-05-22 23:15:39.313115: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-22 23:15:39.485692: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-22 23:15:39.491966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-22 23:15:40.422185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "29536cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m282.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 KB\u001b[0m \u001b[31m74.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m73.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dd7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
