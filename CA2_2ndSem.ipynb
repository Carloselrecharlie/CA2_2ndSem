{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea7399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tarfile\n",
    "import json\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe933cf",
   "metadata": {},
   "source": [
    "## Filtering tweets from monthly .tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b38e724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#keywords to filter the tweets\n",
    "keywords=['gmo','gmos','gm food','gm foods','transgenic', 'transgenics','genetically-modified','genetically modified']\n",
    "\n",
    "# initializing array which will hold the filtered tweets\n",
    "filteredTweets=[]\n",
    "\n",
    "\n",
    "# opening the tar file\n",
    "with tarfile.open('data/TweetDS.tar', 'r') as tar:\n",
    "    \n",
    "    # iterating through all files and directories\n",
    "    for member in tar.getmembers():\n",
    "        \n",
    "        #skipping directories\n",
    "        if member.isdir():\n",
    "            continue\n",
    "        \n",
    "        #looking for the target files json.bz2\n",
    "        if member.name.endswith('.json.bz2'):\n",
    "            \n",
    "            # extracting each file\n",
    "            file = tar.extractfile(member)\n",
    "         \n",
    "            #reading in and decompressing the file\n",
    "            noBz2 = bz2.decompress(file.read()).decode('utf-8')\n",
    "            \n",
    "            # handling each tweet to find the match\n",
    "            for line in noBz2.splitlines():\n",
    "                \n",
    "                #each line is one tweet in json format, therefore loading line by line\n",
    "                tweet = json.loads(line)\n",
    "\n",
    "                # making sure the tweet has the fields 'text' and 'created_at'. If not they'll be skipped\n",
    "                if 'text' not in tweet or 'created_at' not in tweet or tweet['user']['lang'] != 'en':\n",
    "                    continue\n",
    "\n",
    "                # Skipping retweets\n",
    "                if tweet['text'].startswith('RT') and tweet.get('retweeted_status') is not None:\n",
    "                    continue\n",
    "                \n",
    "                # storing fields if present\n",
    "                tweetTxt = tweet['text']\n",
    "                created_at = tweet['created_at']\n",
    "\n",
    "                # Searching matches in text, splitting by words, lower case like keywords\n",
    "                if any(keyword in tweetTxt.lower().split() for keyword in keywords):                \n",
    "                    # Add the matching tweet to the list\n",
    "                    filteredTweets.append({\n",
    "                        'text': tweetTxt,\n",
    "                        'created_at': created_at\n",
    "                    })\n",
    "                    \n",
    "\n",
    "                    \n",
    "#writting the fields from matching tweets to a json output file\n",
    "with open('myTweets.json', 'w') as output:\n",
    "    json.dump(filteredTweets, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bedd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f403b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cacu-VirtualBox.mshome.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ee41f",
   "metadata": {},
   "source": [
    "### Loading data files to pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f5ad2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+\n",
      "|         created_at|                text|\n",
      "+-------------------+--------------------+\n",
      "|2012-01-02 20:09:42|USDA has settled ...|\n",
      "|2012-01-03 02:41:36|Sounds INSANE. GM...|\n",
      "|2012-01-03 06:57:08|Petition: Tell th...|\n",
      "|2012-01-03 12:30:19|Mandatory GMO foo...|\n",
      "|2012-01-03 14:11:44|http://t.co/TI9oL...|\n",
      "|2012-01-03 16:10:27|Want to learn mor...|\n",
      "|2012-01-03 17:53:55|Safety assessment...|\n",
      "|2012-01-03 18:30:38|GMO labeling effo...|\n",
      "|2012-01-03 20:04:57|Leaked: US to Sta...|\n",
      "|2012-01-04 00:22:12|Leaked: US to Sta...|\n",
      "|2012-01-04 13:37:46|@IPIMCIMCIM  GMOs...|\n",
      "|2012-01-05 02:42:31|@Kaybaebaeeee gmo...|\n",
      "|2012-01-05 06:10:28|Apel ws. legislac...|\n",
      "|2012-01-05 09:36:54|Woman files class...|\n",
      "|2012-01-05 16:14:15|Leaked: US to Sta...|\n",
      "|2012-01-05 17:42:20|@harjxo GMO twidd...|\n",
      "|2012-01-05 19:42:18|@songbirdtiff Yup...|\n",
      "|2012-01-05 21:07:46|Label GMO food ca...|\n",
      "|2012-01-06 00:21:13|Transgenic Trees ...|\n",
      "|2012-01-06 01:31:14|Woman Sues Frito-...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#funtion to convert to time stamp with udf\n",
    "def parseDate(date_str):\n",
    "    return datetime.datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "parse_date_udf= udf(parseDate, TimestampType())\n",
    "\n",
    "#reading in the monthly json files\n",
    "tweets= spark.read.json('hdfs://localhost:9000/user1/tweet*.json')\n",
    "\n",
    "# applying udf to column\n",
    "tweets= tweets.withColumn('created_at', parse_date_udf(tweets['created_at']))\n",
    "\n",
    "#sorting by timestamp\n",
    "tweets= tweets.orderBy('created_at')\n",
    "\n",
    "tweets.printSchema();tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e926704d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df84cb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.datetime(2012, 1, 2, 20, 9, 42), text='USDA has settled upon a superb solution: let the GMO industry conduct its own environmental impact tests. http://t.co/Ed5BbDRb')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample\n",
    "tweets.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27f6e3",
   "metadata": {},
   "source": [
    "### Dates without tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "009c4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, date_format, expr, hour\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6840fea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing dates: 31\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2012-02-05|\n",
      "|2012-03-02|\n",
      "|2012-03-03|\n",
      "|2012-03-04|\n",
      "|2012-03-05|\n",
      "|2012-03-06|\n",
      "|2012-03-07|\n",
      "|2012-03-08|\n",
      "|2012-03-09|\n",
      "|2012-03-10|\n",
      "|2012-03-11|\n",
      "|2012-03-12|\n",
      "|2012-03-13|\n",
      "|2012-03-14|\n",
      "|2012-03-15|\n",
      "|2012-03-16|\n",
      "|2012-03-17|\n",
      "|2012-03-18|\n",
      "|2012-03-19|\n",
      "|2012-03-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# oldest and latest dates in df\n",
    "oldestDate= tweets.select(min('created_at')).first()[0].date()\n",
    "latestDate= tweets.select(max('created_at')).first()[0].date()\n",
    "\n",
    "#converting dates to Unix timestamps -> int() cannot be datetime.date\n",
    "oldestDate= int(min_date.strftime(\"%s\"))\n",
    "latestDate= int(max_date.strftime(\"%s\"))\n",
    "\n",
    "\n",
    "#generating the list of dates. 86400 s/day\n",
    "timeRange= spark.range(start=oldestDate, end=latestDate, step=86400).selectExpr(\"to_date(from_unixtime(id)) as date\")\n",
    "\n",
    "\n",
    "\n",
    "#Listing missing dates with anti-join between the date range and the 'created_at' column. \n",
    "missingDates= timeRange.join(tweets, timeRange.date == tweets.created_at.cast(DateType()), 'leftanti') # leftanti selects the rows from the left df that do not have a matching key from the right df\n",
    "\n",
    "\n",
    "print(\"Count of missing dates:\",missingDates.count());missingDates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a18d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd23dff8",
   "metadata": {},
   "source": [
    "#### Checking full hours per day without tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6c290abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "369df44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(hour = hour_of_day)' due to data type mismatch: differing types in '(hour = hour_of_day)' (timestamp and int).;\n'Join LeftOuter, (hour#1371 = hour_of_day#1373)\n:- Project [timestamp_seconds(id#1369L) AS hour#1371]\n:  +- Range (1325534982, 1356574202, step=3600, splits=Some(1))\n+- Aggregate [hour_of_day#1373], [hour_of_day#1373, collect_list(hour_of_day#1373, 0, 0) AS hours#1385]\n   +- Project [created_at#238, text#234, day_of_week#840, month#804, hour(created_at#238, Some(Europe/Dublin)) AS hour_of_day#1373]\n      +- Project [created_at#238, text#234, dayofweek(cast(created_at#238 as date)) AS day_of_week#840, month#804, hour_of_day#821]\n         +- Project [created_at#238, text#234, day_of_week#789, month#804, hour(created_at#238, Some(Europe/Dublin)) AS hour_of_day#821]\n            +- Project [created_at#238, text#234, day_of_week#789, month(cast(created_at#238 as date)) AS month#804]\n               +- Project [created_at#238, text#234, dayofweek(cast(created_at#238 as date)) AS day_of_week#789]\n                  +- Sort [created_at#238 ASC NULLS FIRST], true\n                     +- Project [parseDate(created_at#233) AS created_at#238, text#234]\n                        +- Relation [created_at#233,text#234] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6746/1064040597.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtweetHours\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtweetsWithHour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hour_of_day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hour_of_day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hours'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mallHours\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetHours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallHours\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtweetHours\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour_of_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(hour = hour_of_day)' due to data type mismatch: differing types in '(hour = hour_of_day)' (timestamp and int).;\n'Join LeftOuter, (hour#1371 = hour_of_day#1373)\n:- Project [timestamp_seconds(id#1369L) AS hour#1371]\n:  +- Range (1325534982, 1356574202, step=3600, splits=Some(1))\n+- Aggregate [hour_of_day#1373], [hour_of_day#1373, collect_list(hour_of_day#1373, 0, 0) AS hours#1385]\n   +- Project [created_at#238, text#234, day_of_week#840, month#804, hour(created_at#238, Some(Europe/Dublin)) AS hour_of_day#1373]\n      +- Project [created_at#238, text#234, dayofweek(cast(created_at#238 as date)) AS day_of_week#840, month#804, hour_of_day#821]\n         +- Project [created_at#238, text#234, day_of_week#789, month#804, hour(created_at#238, Some(Europe/Dublin)) AS hour_of_day#821]\n            +- Project [created_at#238, text#234, day_of_week#789, month(cast(created_at#238 as date)) AS month#804]\n               +- Project [created_at#238, text#234, dayofweek(cast(created_at#238 as date)) AS day_of_week#789]\n                  +- Sort [created_at#238 ASC NULLS FIRST], true\n                     +- Project [parseDate(created_at#233) AS created_at#238, text#234]\n                        +- Relation [created_at#233,text#234] json\n"
     ]
    }
   ],
   "source": [
    "minDate= tweets.selectExpr(\"min(created_at)\").first()[0]\n",
    "maxDate= tweets.selectExpr(\"max(created_at)\").first()[0]\n",
    "\n",
    "\n",
    "minTimestamp= int(minDate.timestamp())\n",
    "maxTimestamp= int(maxDate.timestamp())\n",
    "\n",
    "allHours= spark.range(minTimestamp, maxTimestamp, 3600).select(expr(\"timestamp_seconds(id)\").alias('hour'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweetsWithHour= tweets.withColumn('hour_of_day', hour('created_at'))\n",
    "\n",
    "\n",
    "tweetHours= tweetsWithHour.groupBy('hour_of_day').agg(collect_list('hour_of_day').alias('hours'))\n",
    "\n",
    "allHours.join(tweetHours, allHours.hour == tweetHours.hour_of_day, 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae4bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29877e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0c8cc729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sSRIdY1HZCS",
    "outputId": "ab15e23e-0924-42bb-c694-f4ddb58df4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "2023-05-22 23:15:39.313115: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-22 23:15:39.485692: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-22 23:15:39.491966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-22 23:15:40.422185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "29536cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m282.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 KB\u001b[0m \u001b[31m74.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m73.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/hduser/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dd7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
