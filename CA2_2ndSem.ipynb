{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "2aa6b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import json\n",
    "import bz2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0d1f63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['gm','GM','gmo','GMO', 'gmos', 'GMOS','transgenic', 'transgenics','genetically-modified','genetically modified']\n",
    "\n",
    "filteredTweets = []\n",
    "\n",
    "with tarfile.open('data/data.tar', 'r') as tar:\n",
    "    \n",
    "    for member in tar.getmembers():\n",
    "        \n",
    "        if member.isdir():\n",
    "            continue\n",
    "        \n",
    "        if member.name.endswith('.json.bz2'):\n",
    "            \n",
    "            file = tar.extractfile(member)\n",
    "         \n",
    "            noBz2 = bz2.decompress(file.read()).decode('utf-8')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "467c6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.023997\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "for line in noBz2.splitlines():\n",
    "\n",
    "    tweet = json.loads(line)\n",
    "\n",
    "    #skipping tweets without expected fields and not in english \n",
    "    if 'text' not in tweet or 'created_at' not in tweet or tweet['user']['lang'] != 'en':\n",
    "        continue\n",
    "\n",
    "    tweetTxt = tweet['text']\n",
    "    created_at = tweet['created_at']\n",
    "\n",
    "    #matching\n",
    "    if any(keyword in tweetTxt.split() for keyword in keywords):\n",
    "\n",
    "        filteredTweets.append({\n",
    "            'text': tweetTxt,\n",
    "            'created_at': created_at\n",
    "        })\n",
    "\n",
    "\n",
    "#output file\n",
    "with open('testFilter.json', 'w') as output:\n",
    "    json.dump(filteredTweets, output)\n",
    "    \n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056cbafb",
   "metadata": {},
   "source": [
    "2023-05-15 18:21:48.992063 2012/01/16/16/30.json.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "89114cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"GM going to college !! Ughh it's so early :(\",\n",
       "  'created_at': 'Wed Sep 28 04:52:46 +0000 2011'}]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "91cf153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_reply_to_status_id': 118898507196280832,\n",
       " 'text': '@Afuro_Yuya お。江ノ島だ。いいねぇ。生しらすぅぅ',\n",
       " 'in_reply_to_screen_name': 'Afuro_Yuya',\n",
       " 'truncated': False,\n",
       " 'retweeted': False,\n",
       " 'in_reply_to_status_id_str': '118898507196280832',\n",
       " 'source': 'web',\n",
       " 'created_at': 'Wed Sep 28 04:52:59 +0000 2011',\n",
       " 'in_reply_to_user_id_str': '180368616',\n",
       " 'geo': None,\n",
       " 'retweet_count': 0,\n",
       " 'contributors': None,\n",
       " 'id_str': '118911055983419392',\n",
       " 'entities': {'hashtags': [],\n",
       "  'user_mentions': [{'indices': [0, 11],\n",
       "    'name': 'Yuya Kanai',\n",
       "    'id_str': '180368616',\n",
       "    'id': 180368616,\n",
       "    'screen_name': 'Afuro_Yuya'}],\n",
       "  'urls': []},\n",
       " 'place': None,\n",
       " 'coordinates': None,\n",
       " 'user': {'following': None,\n",
       "  'notifications': None,\n",
       "  'profile_background_tile': False,\n",
       "  'contributors_enabled': False,\n",
       "  'verified': False,\n",
       "  'friends_count': 58,\n",
       "  'is_translator': False,\n",
       "  'profile_background_image_url_https': 'https://si0.twimg.com/images/themes/theme9/bg.gif',\n",
       "  'profile_link_color': '2FC2EF',\n",
       "  'listed_count': 1,\n",
       "  'profile_sidebar_border_color': '181A1E',\n",
       "  'profile_image_url': 'http://a0.twimg.com/profile_images/1508690798/________1__normal',\n",
       "  'description': \"Laser&Lighting Engineer\\u3000主に渋谷axxcis、稀にWOMBにてLightingやらせてもらってます。趣味でDJ（Deep House,Techno,House,たまにD'n'B'）\\u3000鶏肉大好き。でもお店は知らない、だって生協の鶏肉が一番美味しいんだもの・・・みつを\",\n",
       "  'favourites_count': 2,\n",
       "  'created_at': 'Sun Apr 18 17:17:59 +0000 2010',\n",
       "  'profile_use_background_image': True,\n",
       "  'default_profile': False,\n",
       "  'show_all_inline_media': False,\n",
       "  'geo_enabled': False,\n",
       "  'time_zone': 'Tokyo',\n",
       "  'profile_background_color': '1A1B1F',\n",
       "  'default_profile_image': False,\n",
       "  'profile_background_image_url': 'http://a1.twimg.com/images/themes/theme9/bg.gif',\n",
       "  'followers_count': 66,\n",
       "  'protected': False,\n",
       "  'url': 'http://www.facebook.com/Ishiyammy',\n",
       "  'profile_image_url_https': 'https://si0.twimg.com/profile_images/1508690798/________1__normal',\n",
       "  'id_str': '134520745',\n",
       "  'lang': 'ja',\n",
       "  'name': '電気屋石山',\n",
       "  'statuses_count': 284,\n",
       "  'profile_text_color': '666666',\n",
       "  'id': 134520745,\n",
       "  'follow_request_sent': None,\n",
       "  'utc_offset': 32400,\n",
       "  'profile_sidebar_fill_color': '252429',\n",
       "  'location': 'Tokyo',\n",
       "  'screen_name': 'ishiyammy'},\n",
       " 'in_reply_to_user_id': 180368616,\n",
       " 'id': 118911055983419392,\n",
       " 'favorited': False}"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d5224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84015360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99ebb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652062535862853633\"\n",
      "            ],\n",
      "            \"id\": \"1652062535862853633\",\n",
      "            \"text\": \"@elonmusk @MuskUniversity The Higgs Boson is the only recorded thing to defy our comprehension of physics.\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652062005136707584\"\n",
      "            ],\n",
      "            \"id\": \"1652062005136707584\",\n",
      "            \"text\": \"@MuskUniversity How does Higgs Boson theory play into your truth?\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652058176832040960\"\n",
      "            ],\n",
      "            \"id\": \"1652058176832040960\",\n",
      "            \"text\": \"@_Higgs_Boson *nods* Don't blame you in the least, and I'm guessing they'll be gone soon...I just really hope the folks that need to see this, DO see it.\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652057651461910528\"\n",
      "            ],\n",
      "            \"id\": \"1652057651461910528\",\n",
      "            \"text\": \"@_Higgs_Boson I'm not even reporting this one...As horrific as it is, I want people to be able to see exactly what *teachers* are saying, and how bad things are...\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652029008199192581\"\n",
      "            ],\n",
      "            \"id\": \"1652029008199192581\",\n",
      "            \"text\": \"How one particle could soon rewrite our laws of the Universe\\n\\n https://t.co/byqRufS6Vz\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652018841105616896\"\n",
      "            ],\n",
      "            \"id\": \"1652018841105616896\",\n",
      "            \"text\": \"Higgs Boson is God Particle! But Why? https://t.co/1crbee8LdB\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652018234709745664\"\n",
      "            ],\n",
      "            \"id\": \"1652018234709745664\",\n",
      "            \"text\": \"@Dr_Clandestine @CERN I obsessed over it, and Angels &amp; Demons (the movie version), released the next year, didn't make it better. Strangely though, I didn't keep up, cos I'm only just learning that the Higgs Boson had been discovered. 2/\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652014640233103382\"\n",
      "            ],\n",
      "            \"id\": \"1652014640233103382\",\n",
      "            \"text\": \"Reminds me of the predicted existence of the elusive Higgs Boson (or \\\"God particle\\\") and the multi-billion dollar international effort to find it. Eventually discovered to global fanfare at @CERN in 2012 with a Nobel Prize for Peter Higgs the next year. https://t.co/xHKI4w4SEd\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652004084910571531\"\n",
      "            ],\n",
      "            \"id\": \"1652004084910571531\",\n",
      "            \"text\": \"All the particles and Bosons hither to discovered are composites of the spacetime quantum. Proton, electron and Higgs Boson are all made of: 10^127, 10^121 and 10^128 quanta, respectively. The whole universe is made of 10^182 Spacetime quanta or the (PQHs, or PQBs).\"\n",
      "        },\n",
      "        {\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1652001269924474891\"\n",
      "            ],\n",
      "            \"id\": \"1652001269924474891\",\n",
      "            \"text\": \"Ik dacht dat het Higgs-boson de massadonor was. https://t.co/8YW0ghuxYM\"\n",
      "        }\n",
      "    ],\n",
      "    \"meta\": {\n",
      "        \"newest_id\": \"1652062535862853633\",\n",
      "        \"next_token\": \"b26v89c19zqg8o3fqkfza64o1bwo0ludqp750iwi4hri5\",\n",
      "        \"oldest_id\": \"1652001269924474891\",\n",
      "        \"result_count\": 10\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import dotenv_values\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # loading API keys\n",
    "# config = dotenv_values(\".env\")\n",
    "\n",
    "# bearer_token = config[\"BEARER_TOKEN\"]\n",
    "\n",
    "# search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "# # Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# # expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "# # query_params = {'query': '(from:twitterdev -is:retweet) OR #twitterdev','tweet.fields': 'author_id', \"max_results\":\"10\"}\n",
    "# query_params = {\n",
    "#     'query' : '\"Higgs Boson\" -is:retweet', # -is:retweet -> avoids retweets\n",
    "#     #\"max_results\":\"100\", # number of results we'll get\n",
    "#     \"start_time\": \"2023-04-24T23:59:00Z\",\n",
    "#     \"end_time\": \"2023-04-28T22:59:00Z\"\n",
    "    \n",
    "# }\n",
    "\n",
    "# # Token authentication method\n",
    "# def bearer_oauth(r):\n",
    "#     r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "#     r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "#     return r\n",
    "\n",
    "# # Setting the query\n",
    "# def connect_to_endpoint(url, params):\n",
    "#     response = requests.get(url, auth=bearer_oauth, params=params) # how we set the request itself with .get\n",
    "    \n",
    "#     return response.json()\n",
    "\n",
    "# # Querying the API\n",
    "# json_response = connect_to_endpoint(search_url, query_params)\n",
    "\n",
    "# # Parsing the response\n",
    "# parsedRes = json.dumps(json_response, indent=4, sort_keys=True, ensure_ascii=True) # indent 4 to look nice indent. ascii to get emoties etc.\n",
    "# print(parsedRes)\n",
    "# # Saving the variable holding tweets to a text file\n",
    "# file = open(\"tweets.json\", \"w\")\n",
    "# file.write(parsedRes)\n",
    "# file.close\n",
    "# # Appending the other variables to the same text file\n",
    "# file = open(\"tweets.json\", \"a\")\n",
    "# file.write('\\n'+parsedRes)\n",
    "# file.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2b25da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19816\\1304795349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaginator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Total number of tweets to retrieve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\montec3\\Anaconda3\\lib\\site-packages\\tweepy\\pagination.py\u001b[0m in \u001b[0;36mflatten\u001b[1;34m(self, limit)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         for response in PaginationIterator(\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         ):\n",
      "\u001b[1;32mC:\\Users\\montec3\\Anaconda3\\lib\\site-packages\\tweepy\\pagination.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# https://twittercommunity.com/t/why-does-timeline-use-pagination-token-while-search-uses-next-token/150963\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         if self.method.__name__ in (\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[1;34m\"search_all_tweets\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"search_recent_tweets\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;34m\"get_all_tweets_count\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "#!pip install tweepy\n",
    "import tweepy\n",
    "\n",
    "paginator = tweepy.Paginator(\n",
    "    json_response,                         # The method you want to use    \n",
    "    exclude=['retweets', 'replies'],       # Some argument for this method\n",
    "    start_time=\"1998-12-01T00:00:00Z\",     # Some argument for this method\n",
    "    #max_results=100                        # How many tweets asked per request\n",
    ")\n",
    "\n",
    "for tweet in paginator.flatten(limit=250): # Total number of tweets to retrieve\n",
    "    print(tweet)\n",
    "\n",
    "with open(r'tweets.txt', 'r') as fp:\n",
    "    \n",
    "    # read all lines using readline()\n",
    "    lines = fp.readlines()\n",
    "    \n",
    "    for row in lines:\n",
    "        # check if string present on a current line\n",
    "        if row.find('text') != -1:  # find() method returns -1 if the value is not found\n",
    "            file1 = open(\"tweetsText.csv\", \"a\")  # append mode\n",
    "            file1.write(row.lstrip().replace('\"text\": \"', ''))\n",
    "            file1.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42210b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
